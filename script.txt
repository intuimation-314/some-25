\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{lipsum}

\title{Nature Loves Symmetry}
\author{Sumit Sah}
\date{}

\begin{document}

\maketitle

\begin{quote}
\textit{“Philosophy is written in this grand book—I mean the universe—which stands continually open to our gaze. But it cannot be understood unless one first learns to comprehend the language and interpret the characters in which it is written. It is written in the language of mathematics, and its characters are triangles, circles, and other geometrical figures, without which it is humanly impossible to understand a single word of it.”}\\
\hfill — \textbf{Galileo Galilei}, \textit{Il Saggiatore} (The Assayer), 1623
\end{quote}


\section*{Part 1: Introducing the Problem through Physical Intuition}

Imagine you tie \textbf{two electrons} to the ends of a string—and let go.

\textit{What happens?}

They fly apart in opposite directions, settling \(180^\circ\) apart along a straight line. That’s not surprising—like charges repel, and the straight line gives them maximum separation.

What if you connect \textbf{three electrons} with strings? You let go.

This time : they arrange themselves into a \textit{perfect equilateral triangle}, each one spaced exactly \(120^\circ\) from the others.

\medskip

\begin{quote}
    \textit{Why do they settle into such precise, symmetric shapes?}
\end{quote}

Of course, the system wants to minimize repulsion, and these symmetric arrangements \textbf{gives maximum separation}—and even more remarkably, they ensure the \textbf{vector sum of all forces is zero i.e, it's in equilibrium}:

\[
\sum_{k=0}^{n-1} \vec{v}_k = 0
\]

\medskip

Let’s go one step further.

What if we place \textbf{four electrons}?

Our intuition might suggest they’d form a square in a plane, spaced \(90^\circ\) apart.

But they don't. We live in 3D world. Instead, they settle into a \textbf{regular tetrahedron} in the 3D space.

Why? Because in three dimensions, tetrahedron provides better spacing—it allows each charge to be as far from the others as possible. More precisely, it achieves \textbf{maximum symmetry}, and ensures that the \textbf{vector sum of all forces is zero in three dimensions}.

The symmetry I’m talking about, interestingly, connects to a well-known classic problem in physics called the \textbf{Thomson problem}.

Instead of tying charges with strings, the Thomson problem asks:

\begin{quote}
    How do multiple electrons arrange themselves on the surface of a sphere to minimize their electrostatic potential energy?
\end{quote}

The answer? They spread out as symmetrically as possible—often forming the vertices of \textbf{Platonic solids}.

For example:
\begin{itemize}
    \item With 4 charges, they form a \textbf{tetrahedron}.
    \item With 6 charges, an \textbf{octahedron}—three on a plane, two above and below.
    \item With 12, an \textbf{icosahedron}, and so on.
\end{itemize}

These beautiful symmetric arrangements are not just visually pleasing—they serve a deeper purpose in physics. They simultaneously:
\begin{itemize}
    \item \textbf{Minimize the total electrostatic potential energy}, and
    \item \textbf{Ensure the vector sum of all forces is zero}, meaning each charge is in perfect \textbf{mechanical equilibrium}.
\end{itemize}

In this video, we’re not solving the Thomson problem explicitly, but we’re using it as a powerful base. It shows us that:

\begin{quote}
\textbf{Nature doesn’t arrange things randomly. It arranges them symmetrically.}
\end{quote}


Symmetry brings balance, stability, and efficiency.
\medskip

\textbf{We see this again and again in nature:}

\begin{itemize}
    \item Electric charges placed on a ring in 2D—or across a sphere in 3D—arrange themselves so that the net electric field at the center becomes zero.

    \item Molecules like methane ($\mathrm{CH_4}$) form a tetrahedral shape, maximizing separation between atoms and resulting in a stable, non-polar configuration.

    \item Water ripples form circular waves, snowflakes grow with six-fold symmetry, and water droplets naturally take on a spherical shape.
\end{itemize}

\textbf{And inspired by nature, we humans design:}
\begin{itemize}
    \item Fan blades and turbines are carefully arranged in perfect radial spacing—so they spin smoothly and stay balanced.

    \item Speaker cones and satellite dishes use symmetry to spread waves evenly in all directions.

    \item Architects design domes and arches not just because they’re beautiful—but because radial symmetry makes them strong and efficient.

\end{itemize}

Symmetry is everywhere. Nature loves it.
And so do we.

But can we prove that symmetry truly brings balance—and equilibrium?

Let's begin with the most familiar setting: the flat, two-dimensional plane.

\section*{Part 2: Revisiting the 2D Case – Starting Simple}

In my last video, we explored a seemingly simple question:

\begin{quote}
    \textit{What is the sum of multiple vectors arranged symmetrically in 2D, spaced equally around a circle?}
\end{quote}

And what we found was beautiful:

\[
\sum_{k=0}^{n-1} \vec{v}_k = 0
\]

The vectors balanced each other perfectly—their sum vanished.

Although the problem seems deceptively simple, and the result feels almost obvious, this concept appears in countless places in physics and nature.

We first approached it intuitively: if you connect these vectors head-to-tail, they form a closed polygon. That loop implies there’s no net displacement—so the sum must be zero.

But mathematicians are not always satisfied with visual reasoning alone. So, I wanted to offer a deeper mathematical proof.

That’s where such an abstract concept like \textbf{Euler’s formula} provided an elegant and precise solution to the problem.


I won't dive too deep into the derivation here—I've included a link in the description below if you'd like to explore it in detail. 

But here's the core idea:

We first place these vectors into the \(xy\)-plane. For simplicity, we assume each vector has magnitude 1.

Each of these vectors can be expressed as the sum of its \(x\)- and \(y\)-components, given by:

\[
\vec{v}_k = \cos\left(\frac{2\pi k}{n}\right)\, \hat{\imath} + \sin\left(\frac{2\pi k}{n}\right)\, \hat{\jmath}
\quad \text{for } k = 0, 1, \dots, n-1
\]

Then, the total vector sum is:

\[
\sum_{k=0}^{n-1} \vec{v}_k =
\left(
\sum_{k=0}^{n-1} \cos\left(\frac{2\pi k}{n}\right),\,
\sum_{k=0}^{n-1} \sin\left(\frac{2\pi k}{n}\right)
\right)
\]
\medskip

To show that the total vector sum is zero, we must prove that both the \(x\)- and \(y\)-components sum to zero simultaneously:

\[
\sum_{k=0}^{n-1} \cos\left(\frac{2\pi k}{n}\right) = 0 \quad \text{and} \quad \sum_{k=0}^{n-1} \sin\left(\frac{2\pi k}{n}\right) = 0
\]

There might be ways to approach this using trigonometry—I haven’t found one myself. 
\textit{If anyone has a clever trigonometric trick, feel free to drop it in the comments below!} 

But there’s a far more elegant way to prove it using \textbf{Euler’s formula}.

\medskip

But even before diving into that, I wanted to verify this computationally. So I wrote a simple Python program to sum up these vectors. And yes—the result is indeed zero. (See the code snippet and screenshot.)

\medskip

Now, We move these group of vectors into the complex plane, where the \(x\)-component corresponds to the real part and the \(y\)-component to the imaginary part.

Thanks to Euler's formula, each vector can be written in exponential form as:

\[
\vec{v}_k = e^{\frac{2\pi i k}{n}} \quad \text{for } k = 0, 1, \dots, n-1
\]

When we take the sum of the exponential, and expand it we get a geometric series:

\[
\sum_{k=0}^{n-1} e^{\frac{2\pi i k}{n}} = 1 + e^\frac{i2\pi}{n} + e^\frac{i4\pi}{n} \cdots + e^\frac{i2(n-1)\pi}{n}= 0
\]
which evaluates to 0.

If you're familiar with the concept of \textit{roots of unity}, the result is even more immediate.

\medskip

This same principle can be extended beyond discrete vectors. We can prove that the total vector sum is zero for a \textbf{continuous distribution} of vectors—like a charged ring, for example. Visually, we can think of it as composed of infinitely many tiny charge elements \( dQ \). For every \( dQ \), there is a directly opposite element \( dQ' \) whose electric field cancels it out. 
Mathematically,

As \( n \to \infty \), the discrete sum

\[
\sum_{k=0}^{n-1} e^{\frac{2\pi i k}{n}}
\]

can be approximated by an integral using Rieman sums:

\[
\sum_{k=0}^{n-1} e^{\frac{2\pi i k}{n}} \quad \longrightarrow \quad \frac{n}{2\pi} \int_{0}^{2\pi} e^{i\theta} \, d\theta
\]

Evaluating the integral:

\[
\int_{0}^{2\pi} e^{i\theta} \, d\theta = 0
\]

So, even in the continuous limit, the result still holds:

\[
\vec{\mathbb{V}} = 0
\]

That was all in two dimensions.

\bigskip

This time, I wanted to go deeper.

\begin{quote}
What happens in \textbf{three dimensions}?
\end{quote}

Just like a ring of charges in 2D, we can imagine a \textbf{charged sphere} in 3D. And just like in 2D we have molecules such as  $BF_3$ that form flat, triangular shapes, In 3D molecules like 
$CH_4$ form beautifully symmetric \textbf{tetrahedrons}.

Of course, such a symmetry gives balance in 3D too. But can we still prove the result?

\begin{quote}

And If symmetry gives us such balance in 2D and 3D... what happens in \textbf{four dimensions}? Or even five?
\end{quote}

Let’s find out.

\section*{Part 3: Why Proving It in 3D Is So Tricky}

Taking the leap from 2D to 3D might sound straightforward—but algebraically proving symmetry becomes surprisingly tricky.

In 2D, things are neat: we only deal with a single angle, \( \theta \), and the coordinates of the vectors on the unit circle are given by:

\[
(\cos \theta, \sin \theta)
\]

As \( \theta \) sweeps from \( 0 \) to \( 2\pi \), the sum becomes:

\[
\sum_{k=0}^{n-1} \left( \cos\left(\frac{2\pi k}{n}\right),\, \sin\left(\frac{2\pi k}{n}\right) \right)
\]

Simple, symmetric, and the result is zero. Beautiful.

\medskip

So can we do something similar in 3D?

In spherical coordinates, each point on the sphere is determined by two angles:
\begin{itemize}
    \item \( \theta \in [0, 2\pi) \): the azimuthal angle (like longitude),
    \item \( \phi \in [0, \pi] \): the polar angle (like latitude).
\end{itemize}

The 3D coordinates of a unit vector pointing in that direction are:

\[
(\sin \phi \cos \theta,\; \sin \phi \sin \theta,\; \cos \phi)
\]

So we can construct a symmetric configuration using a nested double sum over these angles:

\[
\sum_{p=0}^{m} \sum_{k=0}^{n-1}
\left(
    \sin\left(\frac{\pi p}{m}\right)\cos\left(\frac{2\pi k}{n}\right),
    \sin\left(\frac{\pi p}{m}\right)\sin\left(\frac{2\pi k}{n}\right),
    \cos\left(\frac{\pi p}{m}\right)
\right)
= (0, 0, 0)
\]

This indeed gives a total sum of zero—and it follows naturally from the symmetry of sine and cosine sums we already used in 2D.

We can visualize it this way: for a fixed \(\phi\), rotating \(\theta\) through \(2\pi\) generates a ring of vectors evenly spaced around a circle—just like the 2D case. Since the vector sum for each ring vanishes, we repeat this process for multiple values of \(\phi\), stacking rings at different latitudes. 

So intuitively, the total vector sum vanishes because each ring contributes zero.

Mathematically, this means we can factor out the \(\phi\)-dependent terms and apply the same identity we proved earlier:

\[
\sum_{p=0}^{m}
\left(
    \sin\left(\frac{\pi p}{m}\right) \sum_{k=0}^{n-1} \cos\left(\frac{2\pi k}{n}\right),
    \sin\left(\frac{\pi p}{m}\right) \sum_{k=0}^{n-1} \sin\left(\frac{2\pi k}{n}\right),
    n \cdot \cos\left(\frac{\pi p}{m}\right)
\right)
\]

\[
\sum_{k=0}^{n-1}
\left(
    \cos\left(\frac{2\pi k}{n}\right),
    \sin\left(\frac{2\pi k}{n}\right)
\right)
= (0, 0)
\]

To show the \(z\)-component vanishes, we observe:

\[
\sum_{p=0}^{m} \cos\left(\frac{\pi p}{m}\right)
\]

This is a symmetric sum over \(\cos(\phi)\), which also cancels out when sampled evenly over the interval \([0, \pi]\). You can even apply Euler's formula again for a more elegant proof (see attached notes in the document for a deeper dive).

\medskip

So once again, even in 3D, the balanced arrangement of vectors gives us cancellation.

\medskip

But here’s the twist:

This double sum \textit{fails} for small numbers of charges—like 4, 5, or 6. Why? Because those points don’t arrange themselves along latitudes and longitudes. Instead, they settle into beautifully symmetric shapes known as \textbf{Platonic solids}.

This is at the heart of the Thomson problem in physics that we talked about:

\begin{itemize}
    \item For example, with 4 charges, they form a perfect \textbf{tetrahedron}.
\end{itemize}

where angle between any two vectors is about \(109.5^\circ\)— but this configuration that can never be produced by our uniform double sum formula.

You try writing a program to generate coordinates using that method, it won’t match.
Yet, you can still verify visually that the vector sum of Platonic solids is zero:

\begin{itemize}
    \item In a tetrahedron, three of the vectors lie below the \(xy\)-plane. When we project them onto the \(xy\)-plane, we see that they are arranged symmetrically and cancel each other out. The fourth vector, pointing upward, balances the net contribution along the \(z\)-axis—making the total vector sum zero.

    \item For five charges: three in a plane cancel out, and two above and below balance them.
    \item For six: four in a square plane and one each above and below—again, a perfect symmetry.
\end{itemize}

\medskip

But here’s the key insight:

\textbf{There are only five Platonic solids.} As the number of charges increases beyond these, perfectly regular configurations no longer exist.

And this is where the double sum starts to shine.

While it fails for small \(n\), it becomes more accurate as \(n\) grows. The distribution begins to approximate a uniform spread across the sphere. 

In fact, as we increase \(n\) and \(m\), the double sum begins to look more like a Riemann sum—and eventually, it converges to a double integral over the sphere.

\begin{quote}
\textit{In the limit as \(n \to \infty\), the double sum becomes a surface integral—and the result is again zero.}
\end{quote}

\[
\sum_{p=0}^{m} \sum_{k=0}^{n-1}
\left(
    \sin\left(\frac{\pi p}{m}\right)\cos\left(\frac{2\pi k}{n}\right),
    \sin\left(\frac{\pi p}{m}\right)\sin\left(\frac{2\pi k}{n}\right),
    \cos\left(\frac{\pi p}{m}\right)
\right)
\longrightarrow
\int_0^{\pi} \int_0^{2\pi}
\begin{pmatrix}
\sin\phi\cos\theta \\
\sin\phi\sin\theta \\
\cos\phi
\end{pmatrix}
\sin\phi \, d\theta \, d\phi
\]

Evaluating the integral gives:

\[
\int_0^{\pi} \int_0^{2\pi}
\begin{pmatrix}
\sin\phi\cos\theta \\
\sin\phi\sin\theta \\
\cos\phi
\end{pmatrix}
\sin\phi d\theta \, d\phi
=
\begin{pmatrix}
0 \\
0 \\
0
\end{pmatrix}
\]

This confirms that the continuous vector sum over the sphere also vanishes perfectly due to spherical symmetry.

\section*{Part 4: Beyond 3D — Climbing into Higher Dimensions}

Once we’ve proven that in 3D, the natural question arises:

\begin{quote}
\textit{Can we extend this to 4D? 5D? All the way to \( n \) dimensions?}
\end{quote}

Surprisingly, yes. Mathematicians abstract the 2D circle and 3D sphere to higher dimensions: the unit \emph{hypersphere}

\medskip

In four dimensions, points on the surface of a 4D hypersphere can be represented using three angles: \(\phi_1, \phi_2, \theta\). A typical parametrization looks like:

\[
\vec{v}(\phi_1, \phi_2, \theta) =
\begin{pmatrix}
\sin\phi_1 \sin\phi_2 \cos\theta \\
\sin\phi_1 \sin\phi_2 \sin\theta \\
\sin\phi_1 \cos\phi_2 \\
\cos\phi_1
\end{pmatrix}
\]

To simulate a symmetric distribution in 4D, we construct a triple sum:

\[
\sum_{p_1=0}^{m_1} \sum_{p_2=0}^{m_2} \sum_{k=0}^{n-1} \vec{v}\left( \frac{\pi p_1}{m_1}, \frac{\pi p_2}{m_2}, \frac{2\pi k}{n} \right)
\]

Just like in the 3D case, we can fix the outermost angle (say, \(\phi_1 = \frac{\pi p_1}{m_1}\)) and factor out the corresponding scalar terms. The remaining double sum over \(\phi_2\) and \(\theta\) looks exactly like the 3D symmetric case, where we already proved the vector sum is zero.

So the full 4D sum becomes:

\[
\sum_{p_1=0}^{m_1}
\sin\left( \frac{\pi p_1}{m_1} \right)
\cdot
\left(
\sum_{p_2=0}^{m_2}
\sum_{k=0}^{n-1}
\begin{pmatrix}
\sin\left(\frac{\pi p_2}{m_2}\right)\cos\left(\frac{2\pi k}{n}\right) \\
\sin\left(\frac{\pi p_2}{m_2}\right)\sin\left(\frac{2\pi k}{n}\right) \\
\cos\left(\frac{\pi p_2}{m_2}\right)
\end{pmatrix}
\right)
= \vec{0}
\]

Since the entire inner double sum evaluates to \(\vec{0}\) (from the 3D result), multiplying it by any scalar—including \(\sin\left( \frac{\pi p_1}{m_1} \right)\)—still gives zero. Therefore:

\[
\sum_{p_1=0}^{m_1}
\sin\left( \frac{\pi p_1}{m_1} \right)
\cdot \vec{0} = \vec{0}
\]

And as we take the limit \( m_1, m_2, n \to \infty \), the triple sum over the 4D hypersphere becomes a triple integral over its angular parameters:

\[
\int_0^\pi \int_0^\pi \int_0^{2\pi}
\vec{v}(\phi_1, \phi_2, \theta) \cdot
\sin^2\phi_1 \sin\phi_2 \; d\theta \, d\phi_2 \, d\phi_1 = \vec{0}
\]

\medskip

\subsection*{Generalizing to \(n\) Dimensions}

The same logic applies as we move to 5D, 6D, and beyond.

In \(n\)-dimensional space, we use \(n-1\) angles \(\phi_1, \phi_2, \ldots, \phi_{n-2}, \theta\) to describe a point on the unit hypersphere.

We create an \((n-1)\)-nested sum to represent the sum over all directions on the unit hypersphere:

\[
\sum_{p_1=0}^{m_1} \sum_{p_2=0}^{m_2} \cdots \sum_{p_{n-2}=0}^{m_{n-2}} \sum_{k=0}^{n-1} \vec{v}\left(\frac{\pi p_1}{m_1}, \ldots, \frac{\pi p_{n-2}}{m_{n-2}}, k\right)
\]

Where the vector \(\vec{v}\) has coordinates:

\[
\begin{aligned}
\vec{v}(\phi_1, \phi_2, \cdots, \phi_{n-2}, \theta) &= \left(
\begin{array}{c}
\sin(\phi_1) \sin(\phi_2) \cdots \sin(\phi_{n-2}) \cos(\theta) \\
\sin(\phi_1) \sin(\phi_2) \cdots \sin(\phi_{n-2}) \sin(\theta) \\
\sin(\phi_1) \sin(\phi_2) \cdots \cos(\phi_{n-2}) \\
\vdots \\
\cos(\phi_1)
\end{array}
\right)
\end{aligned}
\]

Each angle is discretized as:
\[
\phi_j = \frac{\pi p_j}{m_j}, \quad j = 1 , 2, ..., n-2, \quad \theta = \frac{2\pi k}{n}
\]

Each time, we factor out the outermost sum—fixing one angle—and apply the result from the previous dimension. This gives us a kind of mathematical induction over dimensions:

\begin{quote}
\textit{If the vector sum is zero in \( n \) dimensions, it must also be zero in \( n+1 \) dimensions.}
\end{quote}

This recursive structure reveals that the symmetry-driven vector cancellation holds true in all dimensions.

As \(m_1, m_2, \ldots, m_{n-2}, n \to \infty\), the nested sum becomes a multiple integral over the hypersphere:

\[
\int_0^{\pi} \cdots \int_0^{\pi} \int_0^{2\pi} \vec{v}(\phi_1, \ldots, \phi_{n-2}, \theta) 
\prod_{j=1}^{n-2} \sin^{j}(\phi_j) \, d\theta \, d\phi_{n-2} \cdots d\phi_1 = \vec{0}
\]

This integral evaluates to the zero vector.

\medskip

How amazing is that?

No matter how many dimensions you go up—2D, 3D, 4D, 5D, and so on—\textbf{perfect symmetry always leads to perfect cancellation}.

This is the power of mathematics: even though we cannot visualize symmetry beyond three dimensions, we have a language to describe it precisely. And remarkably, that symmetry still leads to perfect balance.

So, if we were to drop water in an -dimensional world, the droplet would naturally take the shape of an \textbf{n-dimensional hypersphere}.

Because, just like in our world, an \textbf{n-dimensional} hypersphere is more optimal.

\section*{Part 5: A New Kind of Proof — Group Theory and Symmetry}
Up to this point, we’ve used sums, trigonometry, and integrals to understand why symmetric vector arrangements cancel out. Personally, I'm an analysis guy. I’ve always been drawn to this kind of analytic proofs — it’s concrete, step-by-step, and satisfying.

But after my last video, someone left a comment suggesting a completely different approach: using \textbf{rotational symmetry}, inspired by \textbf{group theory}.

Now, I’ll be honest — I didn’t have much background in group theory. I didn’t really know how it connected to any of this. He demonstrated the idea beautifully in 2D. And once I saw it, everything clicked.

\begin{quote}
\textit{Group theory is the mathematical language of symmetry. And I believe this problem — and its solution — is really a beautiful doorway into group theory.}
\end{quote}

So now, let’s switch gears and see how symmetry leads to balance through the lens of \textbf{group theory.}

Let's start simple with 2D, again!

We already saw this vector sum written in exponential form:

\[
S = \sum_{k=0}^{n-1} e^{2\pi i k/n}
\]

Let’s call this sum \( S \). Expanding it gives:

\[
S = 1 + e^{2\pi i / n} + e^{4\pi i / n} + \cdots + e^{2\pi i (n-1)/n}
\]

Each term here represents a vector on the unit circle, equally spaced by angle \( \frac{2\pi}{n} \).

Now let’s multiply both sides by \( e^{2\pi i / n} \). We’ll soon see why:

\[
e^{2\pi i / n} \cdot S = e^{2\pi i / n} + e^{4\pi i / n} + \cdots + e^{2\pi i} = 1
\]

So this new sum is exactly the same as \( S \), just cyclically shifted. That means:

\[
e^{2\pi i / n} \cdot S = S
\]

Geometrically, what we did was rotate each vector by \( \frac{2\pi}{n} \). And yet, the overall vector sum didn’t change.

Now here's the key point:

\[
(e^{2\pi i / n} - 1) \cdot S = 0
\]

For \( n > 1 \), we know \( e^{2\pi i / n} \neq 1 \), so the only solution is:

\[
S = 0
\]

In more group-theory language, this rotation we applied is called a \textbf{group action} — and our vector sum \( S \) is \textbf{invariant} under that action.

And not just that one! We could rotate by \( \frac{4\pi}{n} \), \( \frac{6\pi}{n} \), all the way to \( 2\pi \). Each of these rotations is part of a group — and if applying any group action leaves the vector sum unchanged, then the sum must be the zero vector.

\end{document}
